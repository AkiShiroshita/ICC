---
title: "Yamamoto Study Ver2"
author: "Akihiro Shiroshita"
date: "`r Sys.time()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	fig.height = 4,
	fig.pos = "t",
	message = FALSE,
	warning = FALSE,
	dpi = 350,
	out.extra = ""
)
packages = c("MASS",
             "lmerTest",
             "lme4")
package.check <- lapply(packages, FUN = function(x){
  if (!require(x, character.only = TRUE)){
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})

```

# General simulation-based sample size calculation  

Sample size calculation based on the hypothesis tests can be used only if the sampling distribution of the test statistic is known under null hypothesis and alternative hypothesis. 

Generally, we randomly draw a sample based on the distribution assumption under the null hypothesis, and calculate the test statistic and xx-th percentile of the empirical distribution that is the critical value or type 1 error. Then, generate random samples under the alternative hypothesis, and calculate the test statistic and compare the critical value.    

## Liner mixed-effects model  

This example assumes the sampling distribution under the alternative hypothesis is known.  
Research question: whether a senior rater judges the angle higher than a junior raters.    
Here, we evaluate the angle only at time 1, and the Outcome is assumed to be normally distributed. 

$y=u+B_0 + B_1x + \epsilon$ (where $x$: rater, $u$: subject, $u \sim N(0,\sigma^2_u)$ and residual $\epsilon \sim N(0,\sigma^2_{\epsilon})$)  
$H_0: \beta1 = 0$ and $H_1: \beta1 \neq 0$  

What it the sample size necessary for power of 80%?  

You have to specifiy $\beta_1$ and intrapatient correaltion on the outcome = 0.8 and standard deviation of the outcome = 0.1.  

```{r}
alpha <- 0.05
Bsim <- 500 # number of simulations
effect21 <- 0.01 # beta1 (rater 2 vs rater1 effect) 
effect31 <- -0.02 # beta2 (rater 3 vs rater1 effect) 
effect41 <- -0.03 # beta1 (rater 4 vs rater1 effect)
corr_rater <- 0.8 # intrarater correlation
corr_subject <- 0.8 # intrasubject correlation
beta0 <- 0.6 # beta0 (intercept)
sd_time <- 0.05 # standard deviation of the outcome between time
sd_subject <- 0.15 # standard deviation of the outcome between patient

Nsim <- 50 # number of patients (not number of observations) 
p.value <- c()

rmat_time <- matrix(c(1, corr_subject, corr_subject, 1),
                    nrow = 2,
                    ncol = 2,
                    byrow = TRUE) # intrasubject correlation coefficient

rmat_subject <- matrix(c(1, corr_rater, corr_rater, corr_rater,
                         corr_rater, 1, corr_rater, corr_rater,
                         corr_rater, corr_rater, 1, corr_rater,
                         corr_rater, corr_rater, corr_rater, 1),
                       nrow = 4,
                       ncol = 4,
                       byrow = TRUE) # intrarater correlation coefficient

p.value <- c() 

for (i in 1:Bsim) { # looping over the number of simulation
  
  x <- rep(1:Nsim, each = 8)
  rater <- rep(c(1,1,2,2,3,3,4,4), times = Nsim)
  time <- rep(c(1,2), times = Nsim*4)
  
  v.mat_time <- sd_time*rmat_time # variance covariance matrix between time
  v.mat_subject <- sd_subject*rmat_subject # variance covariance matrix between subject
  
  set.seed(i+Nsim)
  mean.rater <- beta0 * beta0 + effect21*c(0,1,0,0) + effect31*c(0,0,1,0) + effect41*c(0,0,0,1)
  
  y.rater1 <- mvrnorm(Nsim, mean.rater, v.mat_subject)
  y.rater2 <- mvrnorm(Nsim, mean.rater, v.mat_subject)
  
  y.rater <- c(rbind(y.rater1, y.rater2))
  
  y <- c()
  
  for (j in 1:Nsim) {
    tmp <- y.rater[c(2*(j-1)+1,2*(j-1)+2)]
    y.rater_time <- mvrnorm(1, tmp, v.mat_time)
    y <- c(y, y.rater_time)
  }
  
  dataset <- data.frame(y, x, rater, time)
  res <- lmer(y ~ 1 + rater + (1|time/x),
                    data = dataset)
  ## define your aim here (e.g. p-value for rater)
}

pwr.func.res <- sum(ifelse(p.value < alpha, 1, 0))/Bsim
```

```{r}

```

